{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistillBert_SentimentAnalysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2idUL0tGXV47"
      },
      "source": [
        "## Distill Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q37TlII00jm-",
        "outputId": "74e6dbc0-1a7e-41ad-8595-ae1903005b53"
      },
      "source": [
        "#Importing Libraries\n",
        "import tensorflow as tf\n",
        "'''\n",
        "num_gpus_available = len(tf.config.experimental.list_physical_devices('GPU'))\n",
        "print(\"Num GPUs Available: \", num_gpus_available)\n",
        "assert num_gpus_available > 0\n",
        "'''\n",
        "!pip install transformers\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "nGCKiUOa0mvz",
        "outputId": "d6394ae7-0761-4b4d-a509-cf948e87b026"
      },
      "source": [
        "df = pd.read_excel(\"train.xlsx\")\n",
        "df"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>dr. goldberg offers everything i look for in a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Got a letter in the mail last week that said D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>4</td>\n",
              "      <td>I love Olive or Twist. Absolutely love it. As ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1</td>\n",
              "      <td>I've actually had OK experiences at this place...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>4</td>\n",
              "      <td>Girls night had to happen. We decided to go to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1</td>\n",
              "      <td>Owner or manager was very rude when we present...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>5</td>\n",
              "      <td>A beautiful little bar with an exciting \\\"mart...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Polarity                                             Review\n",
              "0            5  dr. goldberg offers everything i look for in a...\n",
              "1            2  Unfortunately, the frustration of being Dr. Go...\n",
              "2            4  Been going to Dr. Goldberg for over 10 years. ...\n",
              "3            4  Got a letter in the mail last week that said D...\n",
              "4            1  I don't know what Dr. Goldberg was like before...\n",
              "...        ...                                                ...\n",
              "1995         4  I love Olive or Twist. Absolutely love it. As ...\n",
              "1996         1  I've actually had OK experiences at this place...\n",
              "1997         4  Girls night had to happen. We decided to go to...\n",
              "1998         1  Owner or manager was very rude when we present...\n",
              "1999         5  A beautiful little bar with an exciting \\\"mart...\n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9NBs5EW7Phq"
      },
      "source": [
        "#redefining polarity\n",
        "df[\"Polarity\"] = df[\"Polarity\"].apply(lambda score: \"positive\" if score >= 3 else \"negative\")\n",
        "df['Polarity'] = df['Polarity'].map({'positive':1, 'negative':0})\n",
        "df = df[[\"Review\", \"Polarity\"]]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtaD7-9-7W10"
      },
      "source": [
        "reviews = df['Review'].values.tolist()\n",
        "labels = df['Polarity'].tolist()"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1JXmfvw-Usx"
      },
      "source": [
        "reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--acc7GM5dek"
      },
      "source": [
        "#divinding test, training and validation data set\n",
        "from sklearn.model_selection import train_test_split\n",
        "training_sentences, validation_sentences, training_labels, validation_labels = train_test_split(reviews, labels, test_size=.2)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx_3h-Nu5dcd"
      },
      "source": [
        "#Assign tokenizer object to the tokenizer class\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_0XMHAM5daN",
        "outputId": "637f7d87-3d8b-4abf-9cf1-d5a2c7ec2fb7"
      },
      "source": [
        "tokenizer([training_sentences[0]], truncation=True, padding=True, max_length=128)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 7224, 1998, 2326, 2001, 2307, 1012, 10439, 25090, 16750, 2020, 2204, 1012, 2005, 4596, 2057, 8823, 1996, 15960, 8616, 4135, 2378, 1998, 1037, 16806, 10893, 1997, 2070, 4066, 1012, 2119, 12595, 2204, 2007, 1996, 15960, 8616, 18964, 2108, 1996, 2190, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubYMLdn05dX7"
      },
      "source": [
        "train_encodings = tokenizer(training_sentences, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(validation_sentences, truncation=True, padding=True)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), training_labels))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(( dict(val_encodings), validation_labels ))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxg47WA35dVp",
        "outputId": "046ebb82-07d7-4d84-9a16-545722fc8885"
      },
      "source": [
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_transform', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_59']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2i9USri5dTF",
        "outputId": "da367b87-9dc4-496c-b1c5-8727f652ec0e"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
        "model.fit(train_dataset.shuffle(100).batch(16),\n",
        "          epochs=2,\n",
        "          batch_size=16,\n",
        "          validation_data=val_dataset.shuffle(100).batch(16))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.9619 WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "100/100 [==============================] - 4756s 47s/step - loss: 0.1105 - accuracy: 0.9619 - val_loss: 0.4236 - val_accuracy: 0.8575\n",
            "Epoch 2/2\n",
            "100/100 [==============================] - 4744s 47s/step - loss: 0.0583 - accuracy: 0.9806 - val_loss: 0.6549 - val_accuracy: 0.8350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc751028590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jm4K12b5dQv",
        "outputId": "7dbb9bbe-b9a5-46e2-af29-6f3c886a3d43"
      },
      "source": [
        "model.save_pretrained(\"./sentiment\")\n",
        "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\"./sentiment\")"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at ./sentiment were not used when initializing TFDistilBertForSequenceClassification: ['dropout_59']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./sentiment and are newly initialized: ['dropout_79']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crXRq_K857_L"
      },
      "source": [
        "test_sentence = \"This is a really good product. I love it\"\n",
        "test_sentence1 = \"This is a really bad product. I hate it\"\n",
        "\n",
        "predict_input = tokenizer.encode(test_sentence,\n",
        "                                 truncation=True,\n",
        "                                 padding=True,\n",
        "                                 return_tensors=\"tf\")"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX0oLUrN578t",
        "outputId": "1de9e0cd-562e-4fd2-c59a-554a0bada51f"
      },
      "source": [
        "tf_output = loaded_model.predict(predict_input)[0]\n",
        "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
        "print(\"Sentence: \",test_sentence)\n",
        "tf.print(\"\\nThe Prediction value :\",tf_prediction[0][1])\n",
        "if (tf_prediction[0][1]>=0.5):\n",
        "  print(\"\\nThe Review is Positive\")\n",
        "else:\n",
        "  print(\"\\nThe Review is Negetive\")"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  This is a really good product. I love it\n",
            "\n",
            "The Prediction value : 0.00230982807\n",
            "\n",
            "The Review is Negetive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pp8wrozIY62",
        "outputId": "991d81f4-5ce1-4712-a1ae-5751b3f44c7a"
      },
      "source": [
        "print(\"Sentence: \",test_sentence1)\n",
        "predict_input = tokenizer.encode(test_sentence1,\n",
        "                                 truncation=True,\n",
        "                                 padding=True,\n",
        "                                 return_tensors=\"tf\")\n",
        "\n",
        "tf_output = loaded_model.predict(predict_input)[0]\n",
        "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
        "\n",
        "tf.print(\"\\nThe Prediction value :\",tf_prediction[0][1])\n",
        "if (tf_prediction[0][1]>=0.5):\n",
        "  print(\"\\nThe Review is Positive\")\n",
        "else:\n",
        "  print(\"\\nThe Review is Negetive\")"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  This is a really bad product. I hate it\n",
            "\n",
            "The Prediction value : 0.00230982807\n",
            "\n",
            "The Review is Negetive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F28ZVeIaKzI2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}